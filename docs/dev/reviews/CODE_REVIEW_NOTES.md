# Code Review Notes ‚Äî ag_network
Date: 2026-01-28
Reviewer(s): Copilot + Kai
Scope: Workspace isolation, CLI trust, evidence discipline (Modules 1-5)
Version/commit: v0.2.0

## Review Goals
- Correctness & trust (workspace isolation, truthful CLI, evidence discipline)
- Performance (identify regressions + hotspots)
- Extensibility (clean boundaries; easy to add tools/skills/workflows)
- Operability (logs/traceability, tests, docs)
- Security & privacy (SSRF, secrets, cross-workspace leakage)

## Invariants (must always hold)
1. WorkspaceContext is required for DB/memory/runs/exports.
2. No cross-workspace reads/writes; workspace_meta mismatch fails fast.
3. CLI labels reflect reality (deterministic vs LLM; retrieved vs generated; cached vs fetched).
4. Evidence snippets (when required) are verbatim substrings of captured sources; verifier enforces.
5. Run folder structure is consistent and complete; validate-run catches corruption.
6. Tests run offline and deterministically; golden runs are stable.

---

# 1) Executive Summary
## Overall assessment
- ‚úÖ Strengths: Clean 5-layer architecture, well-defined WorkspaceContext dataclass, FTS5 search, comprehensive skill contracts, `verify_workspace_id()` guard exists
- ‚ö†Ô∏è Key risks: **Workspace isolation is systematically broken** ‚Äî guard exists but is never auto-invoked; 10+ locations bypass workspace context
- üéØ Highest impact fixes next: (1) Make `SQLiteManager.for_workspace(ctx)` factory that auto-verifies; (2) Audit all 16 instantiation sites; (3) Add `CRMStorage.for_workspace(ctx)`

## Risk register (top 5)
| ID | Risk | Severity (P0/P1/P2) | Area | Evidence | Proposed fix | Test to add |
|---:|------|----------------------|------|----------|--------------|-------------|
| R1 | `verify_workspace_id()` never auto-called | P0 | Storage | `sqlite.py` line 445 defined, never called in `__init__` | Call in `__init__` or provide `for_workspace()` factory | `test_sqlite_rejects_wrong_workspace` |
| R2 | `CRMStorage` has zero workspace awareness | P0 | CRM | `crm/storage.py` entire file | Add `workspace_id` to constructor, store in meta | `test_crm_storage_workspace_isolation` |
| R3 | `_persist_claims` uses global DB | P0 | Kernel | `executor.py:351` `SQLiteManager()` | Pass `db_path` from `RunContext` | `test_claims_written_to_workspace_db` |
| R4 | `SourceIngestor` uses global DB | P0 | Tools | `ingest.py:20` `self.db = SQLiteManager()` | Accept `db_path` in constructor | `test_ingest_writes_to_workspace_db` |
| R5 | CLI commands use global paths | P0 | CLI | `cli.py:578,1340,274` etc. | Use `get_workspace_context(ctx)` throughout | `test_cli_commands_respect_workspace` |

---

# 1.5) Workspace Isolation Invariants (Must Always Be True)

These invariants define correct workspace behavior. **All are currently violated.**

| # | Invariant | Enforcement Point | Current Status |
|---|-----------|-------------------|----------------|
| **I1** | Every `SQLiteManager` instance MUST be bound to a workspace | `__init__` should require `workspace_id` or call `verify_workspace_id()` | ‚ùå **VIOLATED** ‚Äî defaults to global |
| **I2** | Every `CRMStorage` instance MUST be bound to a workspace | Constructor should require `workspace_id` | ‚ùå **VIOLATED** ‚Äî no workspace concept |
| **I3** | `workspace_meta.workspace_id` MUST match expected workspace before any DB operation | `verify_workspace_id()` called on open | ‚ùå **VIOLATED** ‚Äî never auto-called |
| **I4** | Run directories MUST be under `{workspace}/runs/`, never under global `config.runs_dir` | CLI/Orchestrator should use `ctx.runs_dir` | ‚ùå **VIOLATED** ‚Äî 6 global usages |
| **I5** | CRM exports MUST go to `{workspace}/exports/`, never global | CLI should use `ctx.exports_dir` | ‚ùå **VIOLATED** ‚Äî global paths used |
| **I6** | Sources/artifacts MUST be stored in workspace-scoped DB | Ingestor/Executor should use `ctx.db_path` | ‚ùå **VIOLATED** ‚Äî 6 bypass points |
| **I7** | FTS search results MUST be filtered by workspace_id | WHERE clause on all FTS queries | ‚ö†Ô∏è **UNVERIFIED** ‚Äî needs audit |
| **I8** | Opening DB with wrong workspace_id MUST raise `WorkspaceMismatchError` | `verify_workspace_id()` | ‚úÖ **CORRECT** (but never called) |
| **I9** | New DB MUST auto-initialize `workspace_meta` with correct ID | `init_workspace_metadata()` | ‚úÖ **CORRECT** (but never called) |
| **I10** | `WorkspaceContext` MUST be propagated through all layers (CLI ‚Üí Kernel ‚Üí Storage) | Function signatures accept ctx | ‚ùå **VIOLATED** ‚Äî many functions ignore ctx |

### Invariant Enforcement Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DESIRED ENFORCEMENT FLOW                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  CLI Command                                                     ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ  ctx = get_workspace_context(typer_ctx)  ‚óÑ‚îÄ‚îÄ I10: Must exist    ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ  db = SQLiteManager.for_workspace(ctx)   ‚óÑ‚îÄ‚îÄ I1, I3: Factory    ‚îÇ
‚îÇ       ‚îÇ    ‚îî‚îÄ‚ñ∫ verify_workspace_id()     ‚óÑ‚îÄ‚îÄ I8, I9: Auto-guard ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ  crm = CRMStorage.for_workspace(ctx)     ‚óÑ‚îÄ‚îÄ I2: Factory        ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ  run_dir = ctx.runs_dir / run_id         ‚óÑ‚îÄ‚îÄ I4: Scoped path    ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ  Result stored in workspace-scoped DB    ‚óÑ‚îÄ‚îÄ I6: Scoped storage ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# 2) Findings by Subsystem

## 2.1 CLI & UX (Typer) ‚Äî MODULE 1
**Files:**
- `src/agnetwork/cli.py` (2360 lines, 35 commands across 6 groups)

### CLI Command Map (Grouped)

```
ag (main app)
‚îú‚îÄ‚îÄ [BD PIPELINE - Top Level]
‚îÇ   ‚îú‚îÄ‚îÄ research      Company research brief (uses LLM)
‚îÇ   ‚îú‚îÄ‚îÄ outreach      Outreach message drafts (placeholder)
‚îÇ   ‚îú‚îÄ‚îÄ prep          Meeting preparation pack (placeholder)
‚îÇ   ‚îú‚îÄ‚îÄ followup      Post-meeting follow-up (placeholder)
‚îÇ   ‚îú‚îÄ‚îÄ status        Show recent runs ‚ö†Ô∏è USES GLOBAL config.runs_dir
‚îÇ   ‚îú‚îÄ‚îÄ validate-run  Validate run folder integrity
‚îÇ   ‚îî‚îÄ‚îÄ run-pipeline  Full BD pipeline execution (LLM)
‚îÇ
‚îú‚îÄ‚îÄ memory/           Memory management (M5)
‚îÇ   ‚îú‚îÄ‚îÄ rebuild-index Rebuild FTS index
‚îÇ   ‚îî‚îÄ‚îÄ search        Search memory by query
‚îÇ
‚îú‚îÄ‚îÄ crm/              CRM integration (M6)
‚îÇ   ‚îú‚îÄ‚îÄ export-run    Export run to CRM format
‚îÇ   ‚îú‚îÄ‚îÄ export-latest Export latest run
‚îÇ   ‚îú‚îÄ‚îÄ import        Import CRM file ‚ö†Ô∏è USES GLOBAL CRMStorage
‚îÇ   ‚îú‚îÄ‚îÄ list          List CRM contacts ‚ö†Ô∏è USES GLOBAL CRMStorage
‚îÇ   ‚îú‚îÄ‚îÄ search        Search CRM ‚ö†Ô∏è USES GLOBAL CRMStorage
‚îÇ   ‚îî‚îÄ‚îÄ stats         CRM statistics ‚ö†Ô∏è USES GLOBAL CRMStorage
‚îÇ
‚îú‚îÄ‚îÄ sequence/         Outreach sequences (M6)
‚îÇ   ‚îú‚îÄ‚îÄ plan          Create sequence plan ‚ö†Ô∏è USES GLOBAL config.runs_dir
‚îÇ   ‚îú‚îÄ‚îÄ list-templates List available templates
‚îÇ   ‚îú‚îÄ‚îÄ show-template Show template details
‚îÇ   ‚îî‚îÄ‚îÄ templates     (alias for list-templates)
‚îÇ
‚îú‚îÄ‚îÄ workspace/        Workspace management (M7)
‚îÇ   ‚îú‚îÄ‚îÄ create        Create new workspace ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îú‚îÄ‚îÄ list          List workspaces ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îú‚îÄ‚îÄ show          Show workspace details ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îú‚îÄ‚îÄ set-default   Set default workspace ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îî‚îÄ‚îÄ doctor        Diagnose workspace issues ‚úÖ WORKSPACE-AWARE
‚îÇ
‚îú‚îÄ‚îÄ prefs/            Preferences (M7)
‚îÇ   ‚îú‚îÄ‚îÄ show          Show preferences ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îú‚îÄ‚îÄ set           Set preference ‚úÖ WORKSPACE-AWARE
‚îÇ   ‚îî‚îÄ‚îÄ reset         Reset preferences ‚úÖ WORKSPACE-AWARE
‚îÇ
‚îî‚îÄ‚îÄ [WORK/PERSONAL OPS - Top Level]
    ‚îú‚îÄ‚îÄ meeting-summary  Summarize meeting notes (LLM)
    ‚îú‚îÄ‚îÄ status-update    Generate status update (LLM)
    ‚îú‚îÄ‚îÄ decision-log     Log a decision (LLM)
    ‚îú‚îÄ‚îÄ weekly-plan      Create weekly plan (LLM)
    ‚îú‚îÄ‚îÄ errand-list      Create errand list (LLM)
    ‚îî‚îÄ‚îÄ travel-outline   Create travel itinerary (LLM)
```

### Misleading Outputs and Fixes

| Location | Misleading Output | Why Misleading | Fix |
|----------|-------------------|----------------|-----|
| `research:253` | `"üîç Researching..."` | Implies active analysis, but LLM call happens later | `"üîç Starting research run..."` |
| `_print_pipeline_result:760` | `mode_label = "LLM"` | Doesn't indicate if it was a cache hit | Add `"[cached]"` suffix if from cache |
| `outreach:473` | `"üìß Creating outreach..."` | Uses placeholder, not LLM | Add `"[placeholder]"` or implement LLM |
| `prep:516` | `"üìã Preparing..."` | Uses placeholder, not LLM | Add `"[placeholder]"` or implement LLM |
| `followup:559` | `"üìù Creating follow-up..."` | Uses placeholder, not LLM | Add `"[placeholder]"` or implement LLM |
| `status:581` | Shows all runs | No workspace filter | `"üìä Recent runs in {ws_ctx.name}:"` |
| `memory search:976` | `"üîç Searching..."` | Doesn't say if FTS or semantic | `"üîç Searching (FTS)..."` or `"(semantic)"` |
| `crm list:1226` | Lists all contacts | No workspace awareness | Requires CRMStorage fix first |

### 5 Must-Have CLI Regression Tests

```python
# tests/test_cli_regression.py

def test_status_command_respects_workspace(tmp_path):
    """status must only show runs from active workspace, not global."""
    # Create ws1 with run, ws2 with run
    # ag --workspace ws1 status ‚Üí only ws1 runs
    # ag --workspace ws2 status ‚Üí only ws2 runs

def test_crm_commands_use_workspace_storage(tmp_path):
    """CRM import/list/search must use workspace-scoped storage."""
    # Import contact to ws1
    # ag --workspace ws2 crm list ‚Üí must NOT show ws1 contact

def test_research_command_passes_db_path(tmp_path):
    """research must pass workspace db_path to SourceIngestor."""
    # Run research with URL
    # Verify source is in ws.db_path, not global config.db_path

def test_sequence_plan_uses_workspace_runs_dir(tmp_path):
    """sequence plan must use workspace runs_dir."""
    # Create sequence plan in ws1
    # Verify plan.json is in ws1/runs/, not global

def test_pipeline_mode_label_distinguishes_cached(tmp_path):
    """Pipeline output must show [cached] when using cached LLM response."""
    # Run pipeline twice with same inputs
    # Second run output must include [cached] indicator
```

### Observations
- Typer-based CLI with command groups: BD Pipeline, CRM, Workspace, Memory, Sequence, Work/Personal Ops
- `get_workspace_context(ctx)` helper exists and correctly reads `WorkspaceContext` from Typer context
- Many commands do NOT use this helper ‚Äî they access global `config.runs_dir`, `config.db_path` directly
- CLI has `--workspace` flag at top level that sets context, but downstream ignores it
- Several BD commands (`outreach`, `prep`, `followup`) are placeholders, not LLM-powered

### Risks
- P0: `status` command (line 578) uses global `runs_dir` ‚Äî shows runs from wrong workspace
- P0: CRM commands (`crm list`, `crm show`, etc.) use global `CRMStorage()` ‚Äî cross-workspace leakage
- P0: `sequence plan` (line 1340) uses global `runs_dir` ‚Äî plans stored in wrong location
- P0: `crm export` uses global paths ‚Äî exports may include wrong workspace data
- P0: `research` command (line 274) creates `SourceIngestor(run.run_dir)` but no `db_path` ‚Äî writes to global DB
- P1: Some command outputs are misleading (e.g., "Analyzing..." when deterministic, "Retrieved" when generated)
- P1: Placeholder commands don't indicate they're not using LLM

### Recommendations
- Smallest safe change: Audit every command, replace `config.*` with `ctx.obj["workspace"].*`
- Refactor candidates: Create `@require_workspace` decorator that auto-injects context
- UX improvements: Add `[workspace: X]` prefix to outputs; distinguish LLM vs deterministic steps
- Label placeholders: Add `[placeholder]` to outputs for unimplemented commands

### Tests to add
- `test_status_shows_only_workspace_runs` ‚Äî switch workspace, verify isolation
- `test_crm_commands_respect_workspace` ‚Äî create contact in ws1, verify invisible in ws2
- `test_research_writes_to_workspace_db` ‚Äî ingest source, verify in correct DB file
- `test_output_labels_distinguish_llm_vs_placeholder` ‚Äî verify user knows what's real
- `test_all_commands_accept_workspace_flag` ‚Äî parametrized test for all commands

### Notes / TODO
- Full command map created (35 commands across 6 groups)
- Consider splitting `cli.py` into submodules per command group
- Several BD commands need LLM implementation (currently placeholders)

---

## 2.2 Workspace System (Registry, Manifest, Policy, Prefs)
**Files:**
- `src/agnetwork/workspaces/registry.py` (241 lines)
- `src/agnetwork/workspaces/context.py` (145 lines)
- `src/agnetwork/storage/sqlite.py` (lines 388-480 ‚Äî workspace_meta guard)

### WorkspaceContext Construction & Required Usage

**Construction Pattern:**
```python
# Factory method (preferred)
ctx = WorkspaceContext.create(name="bd_work", root_dir=Path("~/.ag/workspaces/bd_work"))

# Direct instantiation
ctx = WorkspaceContext(name="bd_work", workspace_id="uuid-here", root_dir=Path(...))
```

**Derived Paths (auto-computed in `__post_init__`):**
- `ctx.runs_dir` ‚Üí `{root}/runs`
- `ctx.db_path` ‚Üí `{root}/db/workspace.sqlite`
- `ctx.exports_dir` ‚Üí `{root}/exports`
- `ctx.sources_cache_dir` ‚Üí `{root}/sources_cache`

**Required Usage Contract (VIOLATED):**
- Every `SQLiteManager` instantiation MUST pass `db_path=ctx.db_path`
- Every `CRMStorage` instantiation MUST pass workspace-scoped path
- Every run operation MUST use `ctx.runs_dir` not `config.runs_dir`

### workspace_meta Guard: Correctness + Edge Cases

**Guard Implementation** (`sqlite.py:445-480`):
```python
def verify_workspace_id(self, expected_workspace_id: str) -> None:
    if self._workspace_id_verified:
        return  # Fast path: already verified this session

    actual_id = self.get_workspace_id()  # SELECT from workspace_meta

    if actual_id is None:
        # Edge case 1: New DB ‚Äî auto-initialize
        self.init_workspace_metadata(expected_workspace_id)
        return

    if actual_id != expected_workspace_id:
        # Edge case 2: Wrong DB ‚Äî raise exception
        raise WorkspaceMismatchError(expected=expected, actual=actual)

    # Edge case 3: Correct DB ‚Äî update last_accessed
    UPDATE workspace_meta SET last_accessed = ?
```

**Edge Case Analysis:**
| Case | Input State | Expected Behavior | Actual Behavior | Status |
|------|-------------|-------------------|-----------------|--------|
| New DB (no workspace_meta row) | Empty table | Auto-init with given ID | ‚úÖ Correct | OK |
| Correct DB | Row exists, ID matches | Update last_accessed, continue | ‚úÖ Correct | OK |
| Wrong DB | Row exists, ID differs | Raise `WorkspaceMismatchError` | ‚úÖ Correct | OK |
| Migration (renamed workspace) | Old ID in meta | Fail fast | ‚úÖ Correct (intentional) | OK |
| **Guard not called** | Any | Verify on open | ‚ùå Never called | **P0 BUG** |

**Root Issue:** `SQLiteManager.__init__` does NOT call `verify_workspace_id()`. The guard is dead code.

### Observations
- `WorkspaceContext` dataclass is well-designed: `workspace_id`, `db_path`, `runs_dir`, `exports_dir`, `prefs`
- `WorkspaceRegistry` manages creation, listing, switching workspaces
- `verify_workspace_id(conn, expected_id)` exists in `sqlite.py` ‚Äî checks `workspace_meta` table
- The verification logic is CORRECT but is NEVER AUTO-CALLED
- `_workspace_id_verified` flag exists for fast-path optimization

### Risks
- P0: `verify_workspace_id()` is dead code in production ‚Äî defined at `sqlite.py:445` but `__init__` at line 93 does not call it
- P0: No enforcement that `SQLiteManager` must receive workspace context
- P1: `WorkspaceRegistry.get_workspace()` returns context but callers ignore it

### Recommendations
- Smallest safe change: Add `verify_workspace_id()` call inside `SQLiteManager.__init__` when `workspace_id` is provided
- Refactor candidates: `SQLiteManager.for_workspace(ctx: WorkspaceContext)` factory method
- Alternative: Make `db_path` constructor arg required (breaks backward compat)

### Tests to add
- `test_sqlite_rejects_wrong_workspace` ‚Äî open DB with wrong workspace_id, expect exception
- `test_workspace_context_propagates_to_storage` ‚Äî create context, verify DB uses correct path
- `test_registry_enforces_workspace_on_all_ops` ‚Äî integration test for full flow
- `test_new_db_auto_initializes_workspace_meta` ‚Äî verify edge case 1
- `test_workspace_meta_updated_on_access` ‚Äî verify last_accessed updates

---

## 2.3 Storage & SQLite (Schema, Connections, Locking)
**Files:**
- `src/agnetwork/storage/sqlite.py` (1059 lines) ‚Äî main entity storage with FTS5
- `src/agnetwork/storage/memory.py` (line 151) ‚Äî memory API
- `src/agnetwork/crm/storage.py` (772 lines) ‚Äî CRM entities
- `src/agnetwork/crm/adapters/file_adapter.py` (656 lines) ‚Äî file-based CRM adapter

### COMPLETE MAP: Every Place DB Connections Are Created

#### SQLiteManager Instantiations (Production Code)
| Location | Code | Workspace-Aware? | Status |
|----------|------|------------------|--------|
| `sqlite.py:90` | `SQLiteManager(db_path)` ‚Äî static method helper | ‚úÖ Explicit path | OK |
| `sqlite.py:100` | `self.db_path = db_path or config.db_path` ‚Äî default fallback | ‚ùå Falls back to global | **P0** |
| `memory.py:151` | `self.db = SQLiteManager(db_path)` | ‚úÖ Explicit path | OK |
| `cli.py:284` | `db = SQLiteManager(db_path=db_path)` | ‚úÖ Explicit path | OK |
| `cli.py:707` | `db = SQLiteManager(db_path=ws_ctx.db_path)` | ‚úÖ From workspace | OK |
| `cli.py:952` | `db = SQLiteManager(db_path=ws_ctx.db_path)` | ‚úÖ From workspace | OK |
| `cli.py:978` | `db = SQLiteManager(db_path=ws_ctx.db_path)` | ‚úÖ From workspace | OK |
| `cli.py:1559` | `db = SQLiteManager(db_path=context.db_path)` | ‚úÖ From workspace | OK |
| `cli.py:1678` | `db = SQLiteManager(db_path=context.db_path)` | ‚úÖ From workspace | OK |
| `verifier.py:521` | `db = SQLiteManager(db_path=db_path)` | ‚úÖ Explicit path | OK |
| `crm/mapping.py:54` | `self.db = db or SQLiteManager()` | ‚ùå Falls back to global | **P0** |
| `llm_executor.py:826` | `db = SQLiteManager(db_path=ws_ctx.db_path)` | ‚úÖ From workspace | OK |
| `llm_executor.py:829` | `db = SQLiteManager()` ‚Äî fallback branch | ‚ùå Global | **P0** |
| `executor.py:351` | `db = SQLiteManager()` | ‚ùå Global | **P0** |
| `validate.py:312` | `db = SQLiteManager()` | ‚ùå Global | **P0** |
| `ingest.py:20` | `self.db = SQLiteManager()` | ‚ùå Global | **P0** |

#### CRMStorage Instantiations
| Location | Code | Workspace-Aware? | Status |
|----------|------|------------------|--------|
| `crm/storage.py:37` | `CRMStorage(db_path)` ‚Äî static helper | ‚úÖ Explicit path | OK |
| `crm/storage.py:47` | `self.db_path = db_path or config.db_path` | ‚ùå Falls back to global | **P0** |
| `cli.py:1290` | `storage = CRMStorage()` | ‚ùå Global | **P0** |
| `file_adapter.py:72` | `self.storage = storage or CRMStorage()` | ‚ùå Falls back to global | **P0** |

#### Global config.* Usages (runs_dir, db_path)
| Location | Code | Context |
|----------|------|---------|
| `orchestrator.py:46` | `self.run_dir = config.runs_dir / self.run_id` | Fallback when no workspace |
| `cli.py:581` | `config.runs_dir.glob("*")` | `status` command |
| `cli.py:768` | `config.runs_dir / result.run_id` | Output path display |
| `cli.py:1088` | `config.runs_dir.glob("*")` | Listing runs |
| `cli.py:1339` | `config.runs_dir / run_id` | `sequence plan` |
| `crm/mapping.py:76` | `config.runs_dir / run_id` | Run lookup |

### Summary: Bypass Points
**Total SQLiteManager instantiations:** 16
**Workspace-aware:** 10
**Global/bypass:** 6 (P0 bugs)

**Total CRMStorage instantiations:** 4
**Workspace-aware:** 1
**Global/bypass:** 3 (P0 bugs)

**Total config.runs_dir usages:** 6 (all bypasses)

### Observations
- `SQLiteManager` has `workspace_meta` table with `workspace_id` column ‚Äî correct schema
- `verify_workspace_id()` at line 445 does proper check: raises if mismatch
- `__init__` at line 93 accepts optional `db_path` defaulting to `config.db_path` ‚Äî **no workspace verification**
- `CRMStorage` has **ZERO** workspace awareness ‚Äî no `workspace_id` anywhere in file
- `FileCRMAdapter.__init__` at line 68 creates `CRMStorage()` with no args ‚Äî uses global

### Risks
- P0: `SQLiteManager.__init__` does NOT call `verify_workspace_id` ‚Äî guard is dead code
- P0: `CRMStorage` entire class has no workspace concept ‚Äî all CRM data is global
- P0: `FileCRMAdapter` at line 68 uses `self.storage = CRMStorage()` ‚Äî global storage
- P1: FTS index not workspace-scoped ‚Äî search may return cross-workspace results
- P2: Connection pooling not implemented ‚Äî potential performance issue under load

### Recommendations to Eliminate Bypasses

**Phase 1: Factory Methods (non-breaking)**
```python
# Add to SQLiteManager
@classmethod
def for_workspace(cls, ctx: WorkspaceContext) -> "SQLiteManager":
    """Create workspace-bound storage with automatic verification."""
    db = cls(db_path=ctx.db_path)
    db.verify_workspace_id(ctx.workspace_id)
    return db

# Add to CRMStorage
@classmethod
def for_workspace(cls, ctx: WorkspaceContext) -> "CRMStorage":
    """Create workspace-bound CRM storage."""
    return cls(db_path=ctx.db_path, workspace_id=ctx.workspace_id)
```

**Phase 2: Deprecate Default Constructors**
- Add `warnings.warn()` when `SQLiteManager()` called with no args
- Add `warnings.warn()` when `CRMStorage()` called with no args
- Log to help identify remaining bypass points

**Phase 3: Break on Bypass (opt-in)**
- Environment variable: `AG_STRICT_WORKSPACE=1`
- Raises error if workspace context not provided

### Tests That Would Catch Leakage
1. `test_sqlite_manager_verifies_workspace_on_init` ‚Äî constructor calls guard
2. `test_sqlite_rejects_mismatched_workspace` ‚Äî open ws1 DB with ws2 ID ‚Üí error
3. `test_crm_storage_workspace_isolation` ‚Äî create contact in ws1, invisible in ws2
4. `test_file_adapter_uses_workspace_storage` ‚Äî adapter propagates context
5. `test_fts_search_respects_workspace_boundary` ‚Äî search only returns ws results
6. `test_claims_persist_to_workspace_db` ‚Äî executor writes to correct DB
7. `test_ingestor_writes_to_workspace_db` ‚Äî sources go to correct DB
8. `test_cli_status_shows_only_workspace_runs` ‚Äî status filtered by workspace
9. `test_no_global_db_path_in_production_code` ‚Äî grep test for `SQLiteManager()` with no args
10. `test_workspace_meta_populated_on_first_access` ‚Äî new DB gets metadata

---

## 2.4 Memory & FTS Retrieval ‚Äî MODULE 3 (SQLite Patterns)
**Files:**
- `src/agnetwork/storage/sqlite.py` (1059 lines)
- `src/agnetwork/storage/memory.py`
- `src/agnetwork/crm/storage.py` (772 lines)

### SQLite Usage Patterns Audit

#### Connection Lifecycle

**Pattern Used:**
```python
with sqlite3.connect(self.db_path) as conn:
    cursor = conn.cursor()
    # ... operations
    conn.commit()
```

**Observations:**
- ‚úÖ Context manager (`with`) used consistently ‚Äî connections auto-close on exit
- ‚úÖ `close()` method in both `SQLiteManager` and `CRMStorage` handles WAL cleanup
- ‚ö†Ô∏è No persistent connection pooling ‚Äî new connection per operation
- ‚ö†Ô∏è `close()` creates a NEW connection just to disable WAL mode ‚Äî potential issue on Windows

#### Transaction Boundaries

| Location | Transaction Scope | Correct? |
|----------|------------------|----------|
| `_init_db()` | Schema creation | ‚úÖ Implicit tx |
| `rebuild_fts_index()` | DELETE + INSERT | ‚úÖ Single commit |
| `add_source()` | Single INSERT | ‚úÖ OK |
| `upsert_source_from_capture()` | INSERT OR REPLACE | ‚úÖ OK |
| Multi-step pipeline | Multiple operations | ‚ö†Ô∏è No explicit tx boundary |

**Issue:** No explicit `BEGIN TRANSACTION` / `ROLLBACK` for multi-step operations. A failure mid-pipeline could leave partially committed state.

#### File Locking (Windows Safety)

**Concerns:**
1. `close()` method (lines 117-137) opens NEW connection to run PRAGMA commands
2. WAL mode creates `-wal` and `-shm` files that can lock on Windows
3. No `busy_timeout` PRAGMA set ‚Äî concurrent access could fail immediately
4. Tests use `gc.collect()` to force connection cleanup (brittle)

**Windows-Specific Issues Found:**
- `test_memory.py:34` has `close_sqlite_connections()` helper using `gc.collect()` ‚Äî indicates known issues
- `close()` does `PRAGMA journal_mode=DELETE` to switch from WAL ‚Äî may fail if other connections open

#### Indexes and Query Efficiency

**Indexes Present:**
- CRM: `idx_crm_accounts_domain`, `idx_crm_contacts_email`, `idx_crm_activities_run`
- FTS5: `sources_fts`, `artifacts_fts` with triggers for auto-sync

**Missing Indexes (Potential Issues):**
- `sources.run_id` ‚Äî no index, used in queries to filter by run
- `sources.content_hash` ‚Äî no index, used for deduplication
- `claims.artifact_id` ‚Äî no index if table exists
- `workspace_meta` ‚Äî single row, no issue

### Safe DB Usage Checklist

```
‚ñ° Use context manager: `with sqlite3.connect(db_path) as conn:`
‚ñ° Always call .close() when done with SQLiteManager/CRMStorage
‚ñ° Set PRAGMA busy_timeout for concurrent access: `PRAGMA busy_timeout=5000`
‚ñ° Use explicit transactions for multi-step operations: `BEGIN`, `COMMIT`, `ROLLBACK`
‚ñ° On Windows: call close() before file deletion/move
‚ñ° Index columns used in WHERE/JOIN: run_id, content_hash, etc.
‚ñ° FTS5 triggers exist for auto-sync ‚Äî don't INSERT directly to *_fts tables
‚ñ° Use INSERT OR REPLACE for upserts (used correctly)
‚ñ° Verify workspace_id before operations (currently NOT enforced)
```

### Concrete Fixes (Footguns)

#### Fix 1: Add `busy_timeout` PRAGMA (P1)
```python
# In _init_db() after connect:
def _init_db(self) -> None:
    with sqlite3.connect(self.db_path) as conn:
        conn.execute("PRAGMA busy_timeout=5000")  # 5 second wait
        cursor = conn.cursor()
        # ... rest of schema
```

#### Fix 2: Safe Windows `close()` (P1)
```python
def close(self) -> None:
    if self._closed:
        return
    self._closed = True

    # Use check_same_thread=False for cleanup connection
    try:
        with sqlite3.connect(self.db_path, check_same_thread=False) as conn:
            conn.execute("PRAGMA journal_mode=DELETE")
            conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
    except sqlite3.Error:
        pass  # Best effort

    gc.collect()  # Still needed for Python's sqlite3 module
```

#### Fix 3: Add Missing Index for `run_id` (P2)
```python
# In _init_db():
cursor.execute(
    "CREATE INDEX IF NOT EXISTS idx_sources_run_id ON sources(run_id)"
)
cursor.execute(
    "CREATE INDEX IF NOT EXISTS idx_sources_content_hash ON sources(content_hash)"
)
```

### Risks
- P1: No `busy_timeout` ‚Äî concurrent CLI invocations could crash with "database is locked"
- P1: `close()` may fail on Windows if WAL files locked by another process
- P1: Multi-step operations have no rollback on failure
- P2: Missing indexes on `run_id`, `content_hash` could slow queries at scale

### Tests to Add
- `test_concurrent_db_access` ‚Äî two processes writing simultaneously should not crash
- `test_windows_db_cleanup` ‚Äî close() should leave no -wal/-shm files
- `test_transaction_rollback_on_error` ‚Äî partial failure should not corrupt state
- `test_fts_triggers_sync` ‚Äî INSERT to sources should auto-update sources_fts

---

## 2.5 Web Ingestion (Fetch, Clean, Cache, Deep Links) ‚Äî MODULE 4 (Memory/FTS)
**Files:**
- `src/agnetwork/storage/memory.py` (483 lines) ‚Äî MemoryAPI
- `src/agnetwork/storage/sqlite.py` ‚Äî FTS5 schema and triggers

### Module 4: Memory/FTS Retrieval Review

#### FTS Index Rebuild Triggers

| Trigger | When Fired | Auto/Manual |
|---------|------------|-------------|
| `sources_ai` | AFTER INSERT ON sources | ‚úÖ Auto |
| `sources_ad` | AFTER DELETE ON sources | ‚úÖ Auto |
| `sources_au` | AFTER UPDATE ON sources | ‚úÖ Auto |
| `artifacts_ai` | AFTER INSERT ON artifacts | ‚úÖ Auto |
| `artifacts_ad` | AFTER DELETE ON artifacts | ‚úÖ Auto |
| `artifacts_au` | AFTER UPDATE ON artifacts | ‚úÖ Auto |
| `rebuild_fts_index()` | CLI `ag memory rebuild-index` | Manual |

**Observations:**
- ‚úÖ Triggers correctly keep FTS in sync ‚Äî no manual rebuild needed for normal ops
- ‚úÖ `rebuild_fts_index()` exists for recovery (DELETE + re-INSERT)
- ‚ö†Ô∏è No automatic rebuild detection ‚Äî if triggers fail silently, FTS drifts

#### Query Patterns

**FTS5 Query Flow:**
```
User Query ‚Üí _escape_fts_query() ‚Üí search_*_fts() ‚Üí BM25 scoring ‚Üí Results
```

**Issues Found:**
1. `_escape_fts_query()` strips `*` which disables prefix matching ‚Äî may not be intentional
2. `_to_simple_query()` limits to 5 words ‚Äî could miss important context
3. No query logging ‚Äî can't debug why searches return empty

#### Retrieval Mode Recording

**Current State:**
- `EvidenceBundle.query` stores the query string ‚úÖ
- `EvidenceBundle.retrieval_timestamp` stores when retrieved ‚úÖ
- ‚ùå **Missing:** No record of whether FTS or semantic search was used
- ‚ùå **Missing:** No record of result count before limit was applied
- ‚ùå **Missing:** CLI doesn't indicate "Retrieved from memory" vs "No memory hits"

#### Proposed `RetrievalReport` Structure

```python
@dataclass
class RetrievalReport:
    """Observability data for retrieval operations."""

    query: str
    retrieval_mode: Literal["fts5", "semantic", "hybrid"]  # Track method used
    sources_searched: int  # Total in DB
    sources_matched: int   # Before limit
    sources_returned: int  # After limit
    artifacts_searched: int
    artifacts_matched: int
    artifacts_returned: int
    duration_ms: float     # Time to execute
    workspace_id: str      # Which workspace was searched
    timestamp: datetime

    # Debug info
    escaped_query: str     # After escaping
    fallback_used: bool    # Did we fall back to simple query?
```

### Recommended Timing Instrumentation Points

```python
# In MemoryAPI.search_sources():
start = time.perf_counter()
# ... search logic
duration_ms = (time.perf_counter() - start) * 1000
logger.debug(f"FTS search completed in {duration_ms:.2f}ms, {len(results)} hits")
```

**Instrumentation Points:**
1. `search_sources()` entry/exit ‚Äî track FTS latency
2. `search_artifacts()` entry/exit ‚Äî track FTS latency
3. `retrieve_context()` ‚Äî track total retrieval time
4. `_escape_fts_query()` ‚Äî log transformation for debugging
5. `rebuild_fts_index()` ‚Äî log duration and row counts

### Top Likely Performance Regressions

| Risk | Severity | Trigger Condition | Mitigation |
|------|----------|-------------------|------------|
| FTS unbounded scan | P1 | Query matches many rows | Add `LIMIT` clause in SQL (already has `limit` param) |
| No index on `run_id` | P2 | Filter by run in large DB | Add index (see Module 3) |
| Global `_memory_api` singleton | P1 | First call slow; can't switch workspace | Create per-workspace instances |
| Trigger cascade | P2 | Bulk insert triggers many FTS updates | Disable triggers, bulk insert, rebuild |
| Query escaping strips wildcards | P2 | User expects prefix match | Allow `*` in query or add explicit prefix mode |

### Risks
- P1: Global `_memory_api` singleton (line 400) ignores `db_path` after first call ‚Äî workspace leak
- P1: No indication to user whether memory was used ("Retrieved 5 sources" never shown)
- P2: FTS query fallback swallows exceptions silently ‚Äî hard to debug
- P2: `workspace` parameter in search methods is "not yet implemented" ‚Äî comment says so

### Tests to Add
- `test_fts_triggers_sync_on_insert` ‚Äî INSERT source, verify in FTS
- `test_fts_triggers_sync_on_delete` ‚Äî DELETE source, verify removed from FTS
- `test_retrieval_report_captures_timing` ‚Äî verify duration_ms populated
- `test_memory_api_respects_workspace` ‚Äî two workspaces, isolated search results
- `test_search_escaping_preserves_intent` ‚Äî verify prefix match still works

---

## 2.6 Evidence & Verifier (Claims/Evidence Snippets) ‚Äî MODULE 5 (Web/Evidence)
**Files:**
- `src/agnetwork/tools/web/capture.py` (280 lines) ‚Äî URL capture
- `src/agnetwork/tools/web/clean.py` (194 lines) ‚Äî HTML ‚Üí text
- `src/agnetwork/tools/web/fetch.py` (190 lines) ‚Äî HTTP fetching
- `src/agnetwork/tools/web/deeplinks.py` (813 lines) ‚Äî deep link discovery
- `src/agnetwork/eval/verifier.py` (563 lines)
- `src/agnetwork/tools/ingest.py` (129 lines)

### Web Ingestion Pipeline Review

#### Raw/Clean/Meta File Structure
```
sources/
  {slug}__raw.html          # Original HTML
  {slug}__clean.txt         # Extracted text
  {slug}__meta.json         # Metadata (url, hash, title, timestamp)
  deeplinks.json            # M8: discovered links
```

#### UTF-8 Handling
- ‚úÖ `clean.py`: Uses BeautifulSoup which handles encoding
- ‚úÖ `capture.py:149`: `clean_file.read_text(encoding="utf-8")`
- ‚ö†Ô∏è `fetch.py`: Returns `content_bytes` ‚Äî caller must decode
- ‚ö†Ô∏è No explicit charset detection from HTTP headers

#### Caching & Deduplication
- ‚úÖ `SourceCapture._cache` ‚Äî in-memory URL cache per run
- ‚úÖ `_load_existing_cache()` ‚Äî reads from `*__meta.json` files
- ‚úÖ `content_hash` (SHA256) computed on fetch
- ‚úÖ `capture_url(force_refresh=True)` to bypass cache
- ‚ö†Ô∏è No cross-run deduplication (same URL fetched in different runs)

### Failure Modes Matrix

| Failure Mode | Symptom | Current Handling | Recommendation |
|--------------|---------|------------------|----------------|
| **Encoding issues** | Garbled text | BeautifulSoup default | Add charset from headers |
| **Truncation** | Large page cut off | `max_bytes=10MB` in fetch | ‚úÖ OK |
| **JS-heavy pages** | No content extracted | Empty clean text | Log warning; try Playwright |
| **Robots.txt blocked** | 403/429 status | Returns error CapturedSource | ‚úÖ OK |
| **Redirect loops** | Timeout | httpx handles | ‚úÖ OK (max_redirects) |
| **SSL errors** | Connection failed | Exception ‚Üí error result | ‚úÖ OK |
| **Rate limiting** | 429 status | Per-host 1s delay | May need exponential backoff |
| **Deep link explosion** | Too many fetches | `max_total=4` in config | ‚úÖ OK |

### Deep Link Discovery & Agent Constraints

**Deterministic Mode:**
1. Parse HTML for `<a>` tags
2. Score by keyword match (config-driven)
3. Pick top N per category
4. Output to `deeplinks.json`

**Agent Mode (constrained):**
1. Same candidate extraction
2. LLM picks from provided candidates ONLY
3. Cannot invent URLs outside candidate set
4. `method: "agent"` recorded in selection

**Auditability:**
- ‚úÖ `deeplinks.json` records: all candidates, scores, selections, method
- ‚úÖ `DeepLinkSelection.to_dict()` includes reason and method
- ‚ö†Ô∏è No human-readable explanation in CLI output

### Evidence Quote Verification

**Verifier Check Flow:**
```
SkillResult ‚Üí _check_evidence_quotes()
    ‚Üí For each personalization_angle:
        ‚Üí If not is_assumption AND no evidence ‚Üí ERROR
        ‚Üí For each evidence item:
            ‚Üí Load source via source_loader(source_id)
            ‚Üí Check: quote in source_text (exact match)
            ‚Üí Fallback: case-insensitive + whitespace-normalized
            ‚Üí If not found ‚Üí ERROR
```

### Observations (Ingestor/Verifier)
- `Verifier` class validates skill results: schema compliance, evidence snippets, claim consistency
- `create_verifier_with_sources(db_path=...)` factory accepts `db_path` ‚Äî **partially workspace-aware**
- `SourceIngestor` at line 20: `self.db = SQLiteManager()` ‚Äî **uses global DB**
- CLI `research` command at line 274 instantiates `SourceIngestor(run.run_dir)` but passes no `db_path`
- Verifier has `_source_loader` callback pattern ‚Äî good abstraction for source retrieval

### Risks
- P0: `SourceIngestor.__init__` line 20 uses `SQLiteManager()` with no args ‚Äî global DB
- P0: CLI `research` at line 274 doesn't pass workspace DB path to ingestor
- P1: JS-heavy pages return empty content ‚Äî no warning to user
- P1: No charset detection from HTTP headers ‚Äî may mishandle non-UTF8
- P1: Singleton `_verifier` pattern without source loader may miss evidence checks
- P2: No cross-run deduplication ‚Äî same URL fetched repeatedly
- P2: No caching of verification results ‚Äî repeated verification is wasteful

### Recommendations
- Smallest safe change: `SourceIngestor.__init__(self, run_dir, db_path=None)` ‚Äî require or default properly
- Update CLI `research` to pass `ctx.obj["workspace"].db_path`
- Add charset detection from `Content-Type` header
- Log warning when clean text is empty for HTML page

### Tests to add
- `test_ingestor_writes_to_workspace_db` ‚Äî ingest file, verify in correct DB
- `test_verifier_uses_workspace_sources` ‚Äî verify evidence from correct workspace
- `test_research_command_workspace_isolation` ‚Äî CLI integration test
- `test_deep_links_recorded_in_json` ‚Äî audit trail complete
- `test_evidence_tamper_detection` ‚Äî modified quote fails verification
- `test_js_heavy_page_warning` ‚Äî empty extraction logged

---

## 2.7 Kernel Execution (TaskSpec ‚Üí Plan ‚Üí Execute) ‚Äî MODULE 6
**Files:**
- `src/agnetwork/kernel/contracts.py` (241 lines) ‚Äî Skill contracts
- `src/agnetwork/kernel/models.py` (181 lines) ‚Äî TaskSpec, Plan, Step
- `src/agnetwork/kernel/executor.py` (641 lines) ‚Äî Plan execution
- `src/agnetwork/kernel/llm_executor.py` (873 lines) ‚Äî LLM-assisted execution
- `src/agnetwork/kernel/planner.py` ‚Äî Plan generation

### Contract Model Clarity

**Core Types:**
```
TaskSpec            ‚Üí Input specification (task_type, inputs, constraints)
    ‚Üì
Plan                ‚Üí Ordered list of Steps
    ‚Üì
Step                ‚Üí skill_name, inputs, status
    ‚Üì
Skill.run()         ‚Üí (inputs, SkillContext) ‚Üí SkillResult
    ‚Üì
SkillResult         ‚Üí output, artifacts, claims, warnings, metrics
```

**Versioning Fields:**
- `SkillResult.skill_version` ‚Äî skill version string
- `ArtifactRef.metadata` ‚Äî can store schema version
- ‚ö†Ô∏è No global contract version enforced across skill/kernel boundary

### Contract Upgrade Strategy

**Safe Schema Evolution:**
1. **Additive changes** (safe): Add new optional fields to `SkillResult`, `TaskSpec`, etc.
2. **Default values** (safe): New fields must have defaults for backward compat
3. **Deprecation** (safe): Mark old fields, keep working, log warnings
4. **Breaking changes** (requires version bump): Remove fields, change types

**Proposed Versioning:**
```python
class SkillContract:
    CONTRACT_VERSION = "1.0"  # Bump on breaking changes

    @classmethod
    def check_compatibility(cls, result: SkillResult) -> bool:
        # Verify result was produced by compatible contract version
        ...
```

### Skill Registration & Invocation

**Registration Pattern:**
```python
@register_skill("research_brief")
class ResearchBriefSkill:
    def run(self, inputs, context) -> SkillResult: ...
```

**Invocation:**
```python
skill = skill_registry.get(step.skill_name)  # Line 518
if skill:
    result = skill.run(inputs, context)
```

**Issues:**
- ‚ö†Ô∏è Global `skill_registry` singleton ‚Äî not workspace-scoped (but skills are stateless, so OK)
- ‚ö†Ô∏è No skill existence check at plan time ‚Äî fails at execute time

### Error Propagation & Partial Failures

**Current Flow:**
```
Step fails ‚Üí ExecutionResult.add_error() ‚Üí success=False
          ‚Üí Later steps may still run
          ‚Üí Partial artifacts may be written
```

**Issues:**
- P1: No rollback mechanism ‚Äî partial artifacts persist
- P1: No "stop on first error" option
- P2: Errors aggregated but no structured failure report per step

### Brittle Couplings

| Coupling | Location | Risk |
|----------|----------|------|
| `skill_registry` global | `executor.py:57` | Low ‚Äî skills are stateless |
| `TaskType` ‚Üí skill mapping | `planner.py` | Medium ‚Äî implicit; add new task type = update planner |
| `SkillResult.output` is `Any` | `contracts.py:161` | Medium ‚Äî type safety lost |
| `EvidenceBundle` via `Any` | `contracts.py:114` | Low ‚Äî avoids circular import |
| `llm_factory` dependency | `executor.py:127` | Medium ‚Äî must match LLM mode |

### Observations
- `Executor` orchestrates plan execution: parse task ‚Üí build plan ‚Üí execute steps ‚Üí persist artifacts
- `_get_memory_api(db_path=db_path)` at line 220 **correctly** accepts db_path ‚Äî partial workspace awareness
- `_persist_claims()` at line 351 uses `db = SQLiteManager()` ‚Äî **global DB**
- `LLMExecutor` at line 829 has fallback `db = SQLiteManager()` when no explicit path ‚Äî **global DB**
- `RunContext` dataclass carries `run_dir`, `workspace_id` ‚Äî but not always propagated

### Risks
- P0: `_persist_claims` line 351: `SQLiteManager()` with no args ‚Äî claims written to global DB
- P0: `LLMExecutor` line 829 fallback uses global DB ‚Äî skill results may go to wrong workspace
- P1: `RunContext.workspace_id` exists but not consistently used to derive `db_path`
- P1: No rollback on partial failure ‚Äî inconsistent state possible
- P2: No contract version enforcement at skill boundary

### Recommendations
- Smallest safe change: Pass `db_path` from `RunContext` to `_persist_claims` and `LLMExecutor`
- Add helper: `RunContext.get_db_path()` that derives path from workspace_id
- Consider: Make `SQLiteManager()` with no args raise error (force explicit path)
- Add `CONTRACT_VERSION` constant and compatibility check

### Tests to add
- `test_claims_written_to_workspace_db` ‚Äî execute skill, verify claims in correct DB
- `test_llm_executor_respects_workspace` ‚Äî run LLM skill, verify artifacts in workspace
- `test_run_context_propagates_db_path` ‚Äî integration test for full flow
- `test_partial_failure_does_not_corrupt` ‚Äî first skill fails, verify clean state
- `test_new_skill_registration` ‚Äî register skill, execute via plan

---

## 2.8 Skills (BD + work_ops + personal_ops) ‚Äî MODULE 7
**Files:**
- `src/agnetwork/skills/*.py` (BD pipeline: 5 skills)
- `src/agnetwork/skills/work_ops/*.py` (3 skills)
- `src/agnetwork/skills/personal_ops/*.py` (3 skills)
- `src/agnetwork/skills/contracts.py` (re-exports from kernel)

### Skill Inventory

| Skill Name | File | Version | Evidence Discipline |
|------------|------|---------|---------------------|
| **BD Pipeline** |
| `research_brief` | `skills/research_brief.py` | 1.0 | ‚úÖ `source_ids` ‚Üí `SourceRef` |
| `target_map` | `skills/target_map.py` | 1.0 | ‚úÖ `source_ids` ‚Üí `SourceRef` |
| `outreach` | `skills/outreach.py` | 1.0 | ‚ùå No evidence |
| `meeting_prep` | `skills/meeting_prep.py` | 1.0 | ‚ùå No evidence |
| `followup` | `skills/followup.py` | 1.0 | ‚ùå No evidence |
| **Work Ops** |
| `meeting_summary` | `skills/work_ops/meeting_summary.py` | 1.0 | ‚ùå `source_refs=[]` |
| `decision_log` | `skills/work_ops/decision_log.py` | 1.0 | ‚ùå `source_refs=[]` |
| `status_update` | `skills/work_ops/status_update.py` | 1.0 | ‚ùå `source_refs=[]` |
| **Personal Ops** |
| `weekly_plan` | `skills/personal_ops/weekly_plan.py` | 1.0 | ‚ùå `source_refs=[]` |
| `errand_list` | `skills/personal_ops/errand_list.py` | 1.0 | ‚ùå `source_refs=[]` |
| `travel_outline` | `skills/personal_ops/travel_outline.py` | 1.0 | ‚ùå `source_refs=[]` |

### Shared Pattern (Duplication)

Every skill follows identical boilerplate (~80 lines each):
```python
@register_skill("name")
class NameSkill:
    name = "name"
    version = "1.0"

    def __init__(self):
        self.template = self._get_template()

    def _get_template(self) -> Template:
        return Template(template_str)

    def run(self, inputs, context) -> SkillResult:
        # 1. Extract inputs
        # 2. Build data dict
        # 3. Render markdown
        # 4. Build JSON
        # 5. Create claims (mostly empty)
        # 6. Create ArtifactRef (MD + JSON)
        # 7. Return SkillResult
```

### Skill Authoring Guide Checklist

```
‚ñ° Use @register_skill("skill_name") decorator
‚ñ° name class attribute matches decorator argument
‚ñ° version class attribute (semver, e.g. "1.0")
‚ñ° run(inputs: Dict, context: SkillContext) -> SkillResult
‚ñ° Set skill_name and skill_version on result
‚ñ° Artifact naming: static = skill name, dynamic = {skill}_{date}
‚ñ° Both MD and JSON artifacts use SAME name
‚ñ° FACT claims require source_ids ‚Üí SourceRef evidence
‚ñ° Unsourced claims must be ClaimKind.ASSUMPTION
‚ñ° Handle missing inputs with .get() defaults
‚ñ° Include test: test_{skill}_returns_valid_result
```

### Refactor: Extract BaseSkill

```python
class BaseSkill:
    """Base class with common skill patterns."""
    name: str
    version: str = "1.0"
    template_str: str

    def __init__(self):
        self.template = Template(self.template_str)

    def run(self, inputs, context) -> SkillResult:
        data = self.prepare_data(inputs, context)
        markdown = self.template.render(**data)
        json_data = self.prepare_json(data)
        claims = self.extract_claims(data)
        artifacts = self.build_artifacts(markdown, json_data, data)
        return SkillResult(...)

    def prepare_data(self, inputs, context) -> Dict: ...  # Override
    def extract_claims(self, data) -> List[Claim]: ...   # Override
```

### Observations
- All 11 skills use identical boilerplate (Jinja2 template pattern)
- Only `research_brief` and `target_map` properly convert `source_ids` to `SourceRef`
- 9/11 skills have empty `evidence=[]` on all claims
- `contracts.py` re-exports from `kernel.contracts` for convenience
- Artifact naming inconsistent: BD skills use static names, work/personal use dynamic

### Risks
| Risk | Severity | Evidence |
|------|----------|----------|
| 9/11 skills have empty evidence | P1 | All work_ops/personal_ops |
| ~80 lines duplicate boilerplate per skill | P2 | All 11 skill files |
| Inconsistent artifact naming | P2 | Static vs dynamic |
| No input validation | P2 | All skills accept any Dict |
| Only 1 skill has test coverage | P2 | Only `test_skills.py:research_brief` |

### Recommendations
1. **Extract `BaseSkill`** ‚Äî reduce 80-line boilerplate to ~15 lines per skill
2. **Standardize artifact naming** ‚Äî document pattern in authoring guide
3. **Add `_validate_inputs()`** ‚Äî common validation with required/optional keys
4. **Review evidence policy** ‚Äî should work_ops claims cite meeting notes?
5. **Add parametrized tests** ‚Äî test all skills for contract compliance

### Tests to add
```python
@pytest.mark.parametrize("skill_name", ALL_SKILLS)
def test_skill_returns_valid_result(skill_name): ...

@pytest.mark.parametrize("skill_name", ALL_SKILLS)
def test_skill_produces_md_and_json(skill_name): ...

def test_research_brief_facts_have_evidence(): ...

def test_unsourced_claims_marked_assumption(): ...
```

---

## 2.9 Observability & Run Analysis ‚Äî MODULE 8
**Files:**
- `src/agnetwork/orchestrator.py` (200 lines) ‚Äî RunManager
- `src/agnetwork/validate.py` (337 lines) ‚Äî Run folder validation
- `src/agnetwork/kernel/contracts.py` ‚Äî `SkillMetrics`
- `src/agnetwork/kernel/llm_executor.py` ‚Äî `_build_skill_result` (timing)

### Current Logging Infrastructure

| Component | File | Format | Contents |
|-----------|------|--------|----------|
| `agent_worklog.jsonl` | `{run}/logs/` | JSONL | Structured action log: phase, action, status |
| `agent_status.json` | `{run}/logs/` | JSON | Session state: phases, metrics |
| `run.log` | `{run}/logs/` | Plain text | Python logging output |
| `SkillMetrics` | In `SkillResult` | Object | `execution_time_ms`, `input_tokens`, `output_tokens` |

### What CAN Be Reconstructed Today

| Information | Source | Reliability |
|-------------|--------|-------------|
| Which phases ran | `agent_worklog.jsonl` | ‚úÖ Good |
| Action timestamps | `agent_worklog.jsonl` | ‚úÖ Good |
| Phase status | `agent_worklog.jsonl` | ‚úÖ Good |
| Skill execution time | `SkillMetrics.execution_time_ms` | ‚ö†Ô∏è Only in LLM skills |
| Input/output tokens | `SkillMetrics.input_tokens/output_tokens` | ‚ùå Never populated |
| Tool calls made | None | ‚ùå Missing |
| Memory retrieval details | None | ‚ùå Missing |
| LLM prompts/responses | None | ‚ùå Missing |
| Cache hits | None | ‚ùå Missing |

### What's MISSING for Debugging

1. **Tool Call Log** ‚Äî no record of which tools were invoked with what params
2. **Retrieval Details** ‚Äî no record of FTS queries, result counts, selection
3. **LLM Interaction Log** ‚Äî no prompts/responses saved (privacy tradeoff)
4. **Cache Hit/Miss** ‚Äî can't tell if result came from cache
5. **Step-level timing** ‚Äî only skill-level, not sub-operations

### Proposed `trace.jsonl` Schema

```python
@dataclass
class TraceEvent:
    """Single event in the trace log."""
    timestamp: str          # ISO8601
    event_type: Literal["step_start", "step_end", "tool_call", "retrieval", "llm_call", "error"]
    run_id: str
    step_id: Optional[str]  # For step events
    skill_name: Optional[str]
    duration_ms: Optional[float]
    details: Dict[str, Any]  # Event-specific data

# Event types:
# step_start: { inputs, mode }
# step_end: { success, artifact_names, claim_count }
# tool_call: { tool_name, params_summary, result_summary }
# retrieval: { query, mode, sources_matched, sources_returned }
# llm_call: { model, input_tokens, output_tokens, cached }
# error: { error_type, message, stack_trace }
```

### Proposed CLI Commands

```bash
# Explain what happened in a run
ag run explain <run_id>
# Output:
# Run: 20260128_143022__acme__research
# Duration: 12.3s
# Steps:
#   1. research_brief (LLM) - 8.2s - success
#      - Retrieved: 5 sources (FTS)
#      - LLM: claude-3-opus, 1200 tokens
#   2. target_map (deterministic) - 0.1s - success

# Export trace for debugging
ag run trace <run_id> --output trace.jsonl

# Real-time verbose mode
ag research acme --trace --verbose
# Prints step-by-step progress with timing
```

### Observations
- `RunManager` writes structured logs (`agent_worklog.jsonl`, `agent_status.json`)
- `SkillMetrics` has fields for timing and tokens but `input_tokens`/`output_tokens` never populated
- `LLMExecutor._build_skill_result()` calculates `execution_time_ms` ‚úÖ
- Deterministic skills create `SkillMetrics()` with no data
- No tool call logging exists
- No retrieval logging exists
- `deeplinks.py` has `logger.info()` calls but not in structured format

### Risks
| Risk | Severity | Evidence |
|------|----------|----------|
| Can't debug "why did the agent choose X" | P1 | No decision log |
| Can't tell if result came from cache | P1 | No cache indicator |
| `input_tokens`/`output_tokens` always None | P2 | Never assigned |
| No retrieval trace | P2 | FTS queries not logged |
| `run.log` mixes with global Python logs | P2 | Logger named `agnetwork.{run_id}` |

### Recommendations
1. **Add `trace.jsonl`** ‚Äî structured event log per run
2. **Populate token counts** ‚Äî pass through from LLM response
3. **Add `--trace` flag** ‚Äî enable detailed logging mode
4. **Add `ag run explain`** ‚Äî human-readable run summary
5. **Instrument retrieval** ‚Äî log query, mode, counts in `MemoryAPI`

### Tests to add
```python
def test_skill_metrics_populated_for_llm(): ...
def test_trace_captures_tool_calls(): ...
def test_trace_captures_retrieval(): ...
def test_run_explain_output_format(): ...
def test_verbose_mode_prints_progress(): ...
```

---

## 2.10 CI/CD & Packaging ‚Äî MODULE 9
**Files:**
- `.github/workflows/ci.yml` (35 lines)
- `pyproject.toml` (73 lines)
- `requirements-lock.txt` (48 lines)

### Current CI Pipeline

```yaml
# .github/workflows/ci.yml
on: push/PR to main/master
jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    python: 3.11 only
    steps:
      - checkout
      - setup-python 3.11 (with pip cache)
      - pip install -e ".[dev]"
      - ruff check
      - ruff format --check
      - pytest -v --tb=short
```

### Issues Found

| Issue | Severity | Current | Recommended |
|-------|----------|---------|-------------|
| Single Python version | P1 | 3.11 only | Matrix: 3.11, 3.12, 3.13 |
| No Windows testing | P1 | ubuntu-latest only | Add windows-latest |
| No dependency pinning in CI | P2 | `pip install -e ".[dev]"` | Use lockfile or hash-pinned |
| No coverage threshold | P2 | pytest without cov | Add `--cov --cov-fail-under=80` |
| Lockfile has editable installs | P2 | `-e git+...` in lockfile | Pin concrete versions |
| No cache for dependencies | P2 | Only pip cache | Add full deps cache |
| No test isolation | P2 | All tests in one job | Separate lint/test/coverage |

### Dependency Pinning Analysis

**`pyproject.toml` - Loose Pins:**
```toml
dependencies = [
    "typer[all]>=0.9.0",      # Any 0.9+
    "pydantic>=2.0.0",        # Any 2.x
    "httpx>=0.27.0",          # Any 0.27+
]
```

**`requirements-lock.txt` - Problems:**
```
-e git+https://...@b74ea45...#egg=ag_network   # ‚ùå Editable install
pydantic==2.12.5                                # ‚úÖ Pinned
```

**Issues:**
1. Lockfile contains `-e` editable installs ‚Äî not reproducible
2. Lockfile references git commits ‚Äî fragile
3. No separate `requirements.txt` for production vs dev
4. `python_version<'3.11'` conditional in deps but CI only tests 3.11

### Proposed GitHub Actions Matrix

```yaml
name: CI

on:
  push:
    branches: [main]
  pull_request:

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install ruff
      - run: ruff check .
      - run: ruff format --check .

  test:
    needs: lint
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        python: ["3.11", "3.12"]
      fail-fast: false
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: pip
      - name: Install deps
        run: pip install -e ".[dev]"
      - name: Run tests
        run: pytest -v --tb=short --cov=agnetwork --cov-fail-under=80
      - name: Upload coverage
        if: matrix.os == 'ubuntu-latest' && matrix.python == '3.11'
        uses: codecov/codecov-action@v4

  type-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install -e ".[dev]"
      - run: mypy src/agnetwork --ignore-missing-imports
```

### Lockfile Strategy

**Option 1: pip-compile (Recommended)**
```bash
# Generate lockfile from pyproject.toml
pip-compile pyproject.toml -o requirements.lock
pip-compile pyproject.toml --extra dev -o requirements-dev.lock

# Install in CI
pip install -r requirements.lock
pip install -e . --no-deps
```

**Option 2: pip freeze (Current)**
```bash
pip freeze > requirements-lock.txt
# ‚ùå Captures editable installs
# ‚ùå Captures git URLs
```

### Flaky Test Analysis

| Test File | Potential Flakiness | Cause |
|-----------|---------------------|-------|
| `test_memory.py` | ‚ö†Ô∏è Medium | Uses `gc.collect()` for SQLite cleanup |
| `test_web.py` | ‚ö†Ô∏è Medium | May hit network if mocks fail |
| `test_sequence_templates.py` | ‚ö†Ô∏è Low | Datetime comparisons |
| `test_workspace_isolation.py` | ‚ö†Ô∏è Medium | File system timing on Windows |

### Observations
- CI runs on Ubuntu only ‚Äî no Windows coverage
- Single Python version (3.11) ‚Äî no forward compat testing
- No coverage reporting or threshold
- Lockfile has `-e git+...` entries ‚Äî not reproducible
- `mypy` in dev deps but not run in CI
- No separate lint vs test jobs

### Risks
| Risk | Severity | Evidence |
|------|----------|----------|
| Windows-specific bugs undetected | P1 | SQLite WAL, path handling |
| Python 3.12/3.13 breakage undetected | P1 | No matrix testing |
| Dependency drift | P2 | Loose pins, broken lockfile |
| Coverage unknown | P2 | No coverage check |
| Type errors undetected | P2 | mypy not in CI |

### Recommendations
1. **Add matrix testing** ‚Äî Ubuntu + Windows, Python 3.11 + 3.12
2. **Fix lockfile** ‚Äî use pip-compile, remove editable installs
3. **Add coverage threshold** ‚Äî `--cov-fail-under=80`
4. **Add mypy to CI** ‚Äî catch type errors
5. **Split jobs** ‚Äî lint ‚Üí test ‚Üí type-check (parallel)
6. **Add Windows-specific tests** ‚Äî mark with `@pytest.mark.windows`

### Tests to add
```python
@pytest.mark.windows
def test_sqlite_wal_cleanup_windows(): ...

def test_requires_python_311_or_higher(): ...

def test_no_deprecated_imports(): ...
```

---

# 3) Performance Notes
## Benchmarks captured
- Command:
- Dataset:
- Machine:
- Results:

## Hotspots (suspected/confirmed)
- 

## Proposed changes
- 

---

# 4) Security & Privacy Notes
## Threat model assumptions
- Local-first; no automatic external writebacks
- Workspaces isolate work/private/client contexts
- Web fetching is potentially risky (SSRF, internal IPs)

## Findings
- SSRF:
- Secrets handling:
- PII in logs:
- Path traversal:

## Recommended mitigations
- 

---

# 5) Decisions & Follow-ups
- Decisions made during review:
- Follow-up tasks delegated:
- Deferred items (with rationale):

---

# Appendix A ‚Äî Review log
| Time | Module | Area | Files | Notes |
|------|--------|------|-------|-------|
| 2026-01-28 | M1 | CLI | `cli.py` | 35 commands mapped; 5 P0 workspace issues found |
| 2026-01-28 | M2 | Workspace | `workspaces/registry.py`, `context.py` | `verify_workspace_id` is dead code |
| 2026-01-28 | M2 | Storage | `storage/sqlite.py` | Guard exists but never auto-called |
| 2026-01-28 | M2 | CRM Storage | `crm/storage.py`, `file_adapter.py` | Zero workspace awareness |
| 2026-01-28 | M3 | SQLite Patterns | `storage/sqlite.py`, `crm/storage.py` | Safe usage checklist created |
| 2026-01-28 | M4 | Memory/FTS | `storage/memory.py` | RetrievalReport proposed |
| 2026-01-28 | M5 | Web/Evidence | `tools/web/*.py`, `eval/verifier.py` | Failure modes matrix |
| 2026-01-28 | M6 | Kernel | `kernel/*.py` | Contract upgrade strategy |
| 2026-01-28 | M7 | Skills | `skills/**/*.py` | Authoring guide checklist; BaseSkill refactor |
| 2026-01-28 | M8 | Observability | `orchestrator.py`, `validate.py` | trace.jsonl proposal |
| 2026-01-28 | M9 | CI/CD | `.github/workflows/ci.yml`, `pyproject.toml` | Matrix testing proposal |

## Files pending review
- ~~`src/agnetwork/validate.py` line 312 ‚Äî suspected `SQLiteManager()` global usage~~ ‚úÖ Confirmed
- ~~`src/agnetwork/crm/mapping.py` line 54 ‚Äî suspected `self.db = db or SQLiteManager()` pattern~~ ‚úÖ Confirmed
- ~~Memory/FTS subsystem~~ ‚úÖ Reviewed (M4)
- ~~Web ingestion tools~~ ‚úÖ Reviewed (M5)
- ~~Skills implementations~~ ‚úÖ Reviewed (M7)
- ~~Observability/logging~~ ‚úÖ Reviewed (M8)
- ~~CI/CD configuration~~ ‚úÖ Reviewed (M9)

## Module Review Status
| Module | Status | Deliverables |
|--------|--------|--------------|
| M1: CLI | ‚úÖ Complete | Command map, misleading outputs, 5 regression tests |
| M2: Workspace/Storage | ‚úÖ Complete | Invariants, DB connection map, factory methods |
| M3: SQLite Patterns | ‚úÖ Complete | Safe usage checklist, 3 concrete fixes |
| M4: Memory/FTS | ‚úÖ Complete | RetrievalReport proposal, perf risks |
| M5: Web/Evidence | ‚úÖ Complete | Failure modes matrix, evidence flow |
| M6: Kernel | ‚úÖ Complete | Contract upgrade strategy, brittle couplings |
| M7: Skills | ‚úÖ Complete | Authoring guide checklist, BaseSkill refactor |
| M8: Observability | ‚úÖ Complete | trace.jsonl schema, CLI additions |
| M9: CI/CD | ‚úÖ Complete | Matrix proposal, lockfile strategy |
